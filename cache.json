{
  "sources": [
    {
      "title": "Wait But Why",
      "feedUrl": "https://waitbutwhy.com/feed",
      "siteUrl": "https://waitbutwhy.com",
      "articles": []
    },
    {
      "title": "Paul Graham: Essays",
      "feedUrl": "https://rsshub.app/blogs/paulgraham",
      "siteUrl": "http://www.paulgraham.com/articles.html",
      "articles": [
        {
          "id": "http://www.paulgraham.com/own.html",
          "author": null,
          "description": "June 2021\nA few days ago, on the way home from school, my nine year old son\ntold me he couldn't wait to get home to write more of the story he\nwas working on. This made me as happy as anything I've heard him\nsay — not just because he was excited about his story, but because\nhe'd discovered this way of working. Working on a project of your\nown is as different from ordinary work as skating is from walking.\nIt's more fun, but also much more productive.\nWhat proportion of great work has been done by people who were\nskating in this sense? If not all of it, certainly a lot.\nThere is something special about working on a project of your own.\nI wouldn't say exactly that you're happier. A better word would be\nexcited, or engaged. You're happy when things are going well, but\noften they aren't. When I…",
          "link": "http://www.paulgraham.com/own.html",
          "publishedOn": "2022-01-11T19:28:29.978Z",
          "wordCount": 2486,
          "title": "A Project of One's Own"
        },
        {
          "id": "http://www.paulgraham.com/weird.html",
          "author": null,
          "description": "August 2021\nWhen people say that in their experience all programming languages\nare basically equivalent, they're making a statement not about\nlanguages but about the kind of programming they've done.\n99.5% of programming consists of gluing together calls to library\nfunctions. All popular languages are equally good at this. So one\ncan easily spend one's whole career operating in the intersection\nof popular programming languages.\nBut the other .5% of programming is disproportionately interesting.\nIf you want to learn what it consists of, the weirdness of weird\nlanguages is a good clue to follow.\nWeird languages aren't weird by accident. Not the good ones, at\nleast. The weirdness of the good ones usually implies the existence\nof some form of programming that's not just the usual gluing togeth…",
          "link": "http://www.paulgraham.com/weird.html",
          "publishedOn": "2022-01-11T19:28:29.971Z",
          "wordCount": 348,
          "title": "Weird Languages"
        },
        {
          "id": "http://www.paulgraham.com/hwh.html",
          "author": null,
          "description": "June 2021\nIt might not seem there's much to learn about how to work hard.\nAnyone who's been to school knows what it entails, even if they\nchose not to do it. There are 12 year olds who work amazingly hard. And\nyet when I ask if I know more about working hard now than when I\nwas in school, the answer is definitely yes.\nOne thing I know is that if you want to do great things, you'll\nhave to work very hard. I wasn't sure of that as a kid. Schoolwork\nvaried in difficulty; one didn't always have to work super hard to\ndo well. And some of the things famous adults did, they seemed to\ndo almost effortlessly. Was there, perhaps, some way to evade hard\nwork through sheer brilliance? Now I know the answer to that question.\nThere isn't.\nThe reason some subjects seemed easy was that my school had low\ns…",
          "link": "http://www.paulgraham.com/hwh.html",
          "publishedOn": "2022-01-11T19:28:29.960Z",
          "wordCount": 3276,
          "title": "How to Work Hard"
        },
        {
          "id": "http://www.paulgraham.com/smart.html",
          "author": null,
          "description": "October 2021\nIf you asked people what was special about Einstein, most would say\nthat he was really smart. Even the ones who tried to give you a\nmore sophisticated-sounding answer would probably think this first.\nTill a few years ago I would have given the same answer myself. But\nthat wasn't what was special about Einstein. What was special about\nhim was that he had important new ideas. Being very smart was a\nnecessary precondition for having those ideas, but the two are not\nidentical.\nIt may seem a hair-splitting distinction to point out that intelligence\nand its consequences are not identical, but it isn't. There's a big\ngap between them. Anyone who's spent time around universities and\nresearch labs knows how big. There are a lot of genuinely smart\npeople who don't achieve very much.\nI g…",
          "link": "http://www.paulgraham.com/smart.html",
          "publishedOn": "2022-01-11T19:28:29.942Z",
          "wordCount": 1406,
          "title": "Beyond Smart"
        },
        {
          "id": "http://www.paulgraham.com/goodtaste.html",
          "author": null,
          "description": "November 2021\n(This essay is derived from a talk at the Cambridge Union.)\nWhen I was a kid, I'd have said there wasn't. My father told me so.\nSome people like some things, and other people like other things,\nand who's to say who's right?\nIt seemed so obvious that there was no such thing as good taste\nthat it was only through indirect evidence that I realized my father\nwas wrong. And that's what I'm going to give you here: a proof by\nreductio ad absurdum. If we start from the premise that there's no\nsuch thing as good taste, we end up with conclusions that are\nobviously false, and therefore the premise must be wrong.\nWe'd better start by saying what good taste is. There's a narrow\nsense in which it refers to aesthetic judgements and a broader one\nin which it refers to preferences of any kin…",
          "link": "http://www.paulgraham.com/goodtaste.html",
          "publishedOn": "2022-01-11T19:28:29.928Z",
          "wordCount": 1123,
          "title": "Is There Such a Thing as Good Taste?"
        }
      ]
    },
    {
      "title": "LessWrong",
      "feedUrl": "https://www.lesswrong.com/feed.xml?view=curated-rss",
      "siteUrl": "https://www.lesswrong.com",
      "articles": [
        {
          "id": "puYfAEJJomeodeSsi",
          "author": "Elizabeth",
          "description": "Published on January 3, 2022 9:10 PM GMT\n\nI aspire to be a person who does good things, and who is capable of doing hard things in service of that. This is a plan to test that capacity.\nI haven’t been in a battle, but if you gave me the choice between dying in battle and slowly starving to death, I would immediately choose battle. Battles are scary but they are short and then they are over.\nIf you gave me a chance to starve to death to generate some sufficiently good outcome, like saving millions of people from starvation, I think I would do it, and I would be glad to have the opportunity. It would hurt, but only for a few weeks, and in that time I could comfort myself with the warm glow of how good this was for other people.\nIf you gave me a chance to save millions of people by starving, …",
          "link": "https://www.lesswrong.com/posts/puYfAEJJomeodeSsi/an-observation-of-vavilov-day",
          "publishedOn": "2022-01-09T07:15:11.000Z",
          "wordCount": 3021,
          "title": "An Observation of Vavilov Day"
        },
        {
          "id": "pv7Qpu8WSge8NRbpB",
          "author": "nostalgebraist",
          "description": "Published on November 26, 2021 11:08 PM GMT\n\nwhat this post is\nThe following is an incomplete draft, which I'm publishing now because I am unlikely to ever finish writing it.\nI no longer fully endorse all the claims in the post.  (In a few cases, I've added a note to say this explicitly.)  However, there are some arguments in the post that I still endorse, and which I have not seen made elsewhere.\nThis post is the result of me having lots of opinions about LM scaling, at various times in 2021, which were difficult to write down briefly or independently of one another.  This post, originally written in July 2021, is the closest I got to writing them all down in one place.\n-nost, 11/26/21\n\n0. caveat\nThis post will definitely disappoint you.\nOr, anyway, it will definitely disappoint me.  I kn…",
          "link": "https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally",
          "publishedOn": "2022-01-06T08:13:17.000Z",
          "wordCount": 16109,
          "title": "larger language models may disappoint you [or, an eternally unfinished draft]"
        },
        {
          "id": "qHCDysDnvhteW7kRd",
          "author": "paulfchristiano",
          "description": "Published on December 14, 2021 8:09 PM GMT\n\nARC has published a report on Eliciting Latent Knowledge, an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are working on, how they fit into our plan for solving alignment in the worst case, and our research methodology.\nThe core difficulty we discuss is learning how to map between an AI’s model of the world and a human’s model. This is closely related to ontology identification (and other similar statements). Our main contribution is to present many possible approaches to the problem and a more precise discussion of why it seems to be difficult and important.\nThe report is available here as a google document. If you're excited about this research, we're hiring!\nQ&A\nWe're particularly excited about answering questions posted here throughout December. We welcome any questions no matter how basic or confused; we would love to help people understand what research we’re doing and how we evaluate progress in enough detail that they could start to do it themselves.\nThanks to María Gutiérrez-Rojas for the illustrations in this piece (the good ones, blame us for the ugly diagrams). Thanks to Buck Shlegeris, Jon Uesato, Carl Shulman, and especially Holden Karnofsky for helpful discussions and comments.\n\nDiscuss",
          "link": "https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge",
          "publishedOn": "2022-01-03T11:31:42.000Z",
          "wordCount": 13276,
          "title": "ARC's first technical report: Eliciting Latent Knowledge"
        },
        {
          "id": "zFhhDCxz87yKwqYQf",
          "author": "Zvi",
          "description": "Published on December 28, 2021 5:10 PM GMT\n\nA year and a half ago, I wrote a post called Covid-19: My Current Model. Since then things have often changed, and we have learned a lot. It seems like high time for a new post of this type.\nNote that this post mostly does not justify and explain its statements. I document my thinking, sources and analysis extensively elsewhere, little of this should be new.\nThis post combines the basic principles from my original post, which mostly still stand, with my core model for Omicron. I’ll summarize and update the first post, then share my current principles for Omicron and how to deal with and think about it.\nThere’s a lot of different things going on, so this will likely be incomplete, but hopefully it will prove useful. The personally useful executive…",
          "link": "https://www.lesswrong.com/posts/zFhhDCxz87yKwqYQf/omicron-my-current-model",
          "publishedOn": "2021-12-30T09:28:57.000Z",
          "wordCount": 7777,
          "title": "Omicron: My Current Model"
        },
        {
          "id": "nNqXfnjiezYukiMJi",
          "author": "HoldenKarnofsky",
          "description": "Published on December 23, 2021 4:15 PM GMT\n\nThe \"biological anchors\" method for forecasting transformative AI is the biggest non-trust-based input into my thinking about likely timelines for transformative AI. While I'm sympathetic to parts of Eliezer Yudkowsky's recent post on it, I overall disagree with the post, and think it's easy to get a misimpression of the \"biological anchors\" report (which I'll abbreviate as \"Bio Anchors\") - and Open Philanthropy's take on it - by reading it. \nThis post has three sections:\n\nMost of Eliezer's critique seems directed at assumptions the report explicitly does not make about how transformative AI will be developed, and more broadly, about the connection between its (the report's) compute estimates and all-things-considered AI timelines. One way of put…",
          "link": "https://www.lesswrong.com/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors",
          "publishedOn": "2021-12-27T02:50:32.000Z",
          "wordCount": 14534,
          "title": "Reply to Eliezer on Biological Anchors"
        },
        {
          "id": "sjRG35aq5fosJ6mdG",
          "author": "Duncan_Sabien",
          "description": "Published on November 30, 2021 7:01 PM GMT\n\nAuthor's note: this essay was originally published elsewhere in 2019. It's now being permanently rehosted at this link. I'll be rehosting a small number of other upper-quintile essays from my past writing over the coming weeks.\n\nQuite often, I will make an agreement, and then find myself regretting it. I’ll commit to spending a certain amount of hours helping someone with their problem, or I’ll agree to take part in an outing or a party or a project, or I’ll trade some item for a certain amount of value in return, and then later find that my predictions about how I would feel were pretty far off, and I'm unhappy.\nSometimes, I just suck it up and stick it out. But sometimes I renege on the agreement. In this essay, I’d like to lay out a model for …",
          "link": "https://www.lesswrong.com/posts/sjRG35aq5fosJ6mdG/reneging-prosocially",
          "publishedOn": "2021-12-22T22:03:02.000Z",
          "wordCount": 6760,
          "title": "Reneging Prosocially"
        },
        {
          "id": "3L46WGauGpr7nYubu",
          "author": "johnswentworth",
          "description": "Published on December 10, 2021 11:41 PM GMT\n\nThis is a high-level overview of the reasoning behind my research priorities, written as a Q&A.\nWhat’s your plan for AI alignment?\nStep 1: sort out our fundamental confusions about agency\nStep 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)\nStep 3: …\nStep 4: profit!\n… and do all that before AGI kills us all.\nThat sounds… awfully optimistic. Do you actually think that’s viable?\nBetter than a 50/50 chance of working in time.\nDo you just have really long timelines?\nNo. My median is maybe 10-15 years, though that’s more a gut estimate based on how surprised I was over the past decade rather than a carefully-considered analysis. (I wouldn’t be shocked by another AI winter, especially on an ins…",
          "link": "https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan",
          "publishedOn": "2021-12-19T04:20:49.000Z",
          "wordCount": 14533,
          "title": "The Plan"
        },
        {
          "id": "GzzJZmqxcqg5KFf8r",
          "author": "Connor_Flexman",
          "description": "Published on December 8, 2021 11:13 PM GMT\n\n6 months ago I wrote about how 30-year-olds should basically go back to normal and no longer take many COVID precautions.\nThe holidays make this policy somewhat worse, because you can infect your family members. Further, vaccine efficacy has waned about 3x for most people since it’s been about six months since the vaccine wave. Some people have expressed concern about solstice in particular, which seems to turn out relatively safe. So let’s examine how you might navigate this.\nThe key takeaway is that a 1% chance of having COVID, which is about the base rate of COVID in the US, costs older relatives a few days of life if you pass it on to them. Thus, you should probably take a few easy actions beforehand to bring this down to a small number of ho…",
          "link": "https://www.lesswrong.com/posts/GzzJZmqxcqg5KFf8r/covid-and-the-holidays",
          "publishedOn": "2021-12-15T18:43:19.000Z",
          "wordCount": 5151,
          "title": "COVID and the holidays"
        }
      ]
    }
  ],
  "cliVersion": "1.12.0"
}