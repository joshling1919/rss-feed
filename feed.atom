<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://joshling1919.github.io/rss-feed/index.html</id>
    <title>rss feed</title>
    <updated>2022-01-11T19:54:44.916Z</updated>
    <generator>osmosfeed 1.12.0</generator>
    <link rel="alternate" href="https://joshling1919.github.io/rss-feed/index.html"/>
    <link rel="self" href="https://joshling1919.github.io/rss-feed/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Future ML Systems Will Be Qualitatively Different]]></title>
        <id>pZaPhGg2hmmPwByHc</id>
        <link href="https://www.lesswrong.com/posts/pZaPhGg2hmmPwByHc/future-ml-systems-will-be-qualitatively-different"/>
        <updated>2022-01-11T19:50:12.000Z</updated>
        <summary type="html"><![CDATA[Published on January 11, 2022 7:50 PM GMT

In 1972, the Nobel prize-winning physicist Philip Anderson wrote the essay "More Is Different". In it, he argues that quantitative changes can lead to qualitatively different and unexpected phenomena. While he focused on physics, one can find many examples of More is Different in other domains as well, including biology, economics, and computer science. Some examples of More is Different include:
Uranium. With a bit of uranium, nothing special happens; with a large amount of uranium packed densely enough, you get a nuclear reaction.
DNA. Given only small molecules such as calcium, you can’t meaningfully encode useful information; given larger molecules such as DNA, you can encode a genome.
Water. Individual water molecules aren’t wet. Wetness only…]]></summary>
        <author>
            <name>jsteinhardt</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Project of One's Own]]></title>
        <id>http://www.paulgraham.com/own.html</id>
        <link href="http://www.paulgraham.com/own.html"/>
        <updated>2022-01-11T19:28:29.978Z</updated>
        <summary type="html"><![CDATA[June 2021
A few days ago, on the way home from school, my nine year old son
told me he couldn't wait to get home to write more of the story he
was working on. This made me as happy as anything I've heard him
say — not just because he was excited about his story, but because
he'd discovered this way of working. Working on a project of your
own is as different from ordinary work as skating is from walking.
It's more fun, but also much more productive.
What proportion of great work has been done by people who were
skating in this sense? If not all of it, certainly a lot.
There is something special about working on a project of your own.
I wouldn't say exactly that you're happier. A better word would be
excited, or engaged. You're happy when things are going well, but
often they aren't. When I…]]></summary>
        <author>
            <name>Paul Graham: Essays</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weird Languages]]></title>
        <id>http://www.paulgraham.com/weird.html</id>
        <link href="http://www.paulgraham.com/weird.html"/>
        <updated>2022-01-11T19:28:29.971Z</updated>
        <summary type="html"><![CDATA[August 2021
When people say that in their experience all programming languages
are basically equivalent, they're making a statement not about
languages but about the kind of programming they've done.
99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one's whole career operating in the intersection
of popular programming languages.
But the other .5% of programming is disproportionately interesting.
If you want to learn what it consists of, the weirdness of weird
languages is a good clue to follow.
Weird languages aren't weird by accident. Not the good ones, at
least. The weirdness of the good ones usually implies the existence
of some form of programming that's not just the usual gluing togeth…]]></summary>
        <author>
            <name>Paul Graham: Essays</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Work Hard]]></title>
        <id>http://www.paulgraham.com/hwh.html</id>
        <link href="http://www.paulgraham.com/hwh.html"/>
        <updated>2022-01-11T19:28:29.960Z</updated>
        <summary type="html"><![CDATA[June 2021
It might not seem there's much to learn about how to work hard.
Anyone who's been to school knows what it entails, even if they
chose not to do it. There are 12 year olds who work amazingly hard. And
yet when I ask if I know more about working hard now than when I
was in school, the answer is definitely yes.
One thing I know is that if you want to do great things, you'll
have to work very hard. I wasn't sure of that as a kid. Schoolwork
varied in difficulty; one didn't always have to work super hard to
do well. And some of the things famous adults did, they seemed to
do almost effortlessly. Was there, perhaps, some way to evade hard
work through sheer brilliance? Now I know the answer to that question.
There isn't.
The reason some subjects seemed easy was that my school had low
s…]]></summary>
        <author>
            <name>Paul Graham: Essays</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Smart]]></title>
        <id>http://www.paulgraham.com/smart.html</id>
        <link href="http://www.paulgraham.com/smart.html"/>
        <updated>2022-01-11T19:28:29.942Z</updated>
        <summary type="html"><![CDATA[October 2021
If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn't what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, but the two are not
identical.
It may seem a hair-splitting distinction to point out that intelligence
and its consequences are not identical, but it isn't. There's a big
gap between them. Anyone who's spent time around universities and
research labs knows how big. There are a lot of genuinely smart
people who don't achieve very much.
I g…]]></summary>
        <author>
            <name>Paul Graham: Essays</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is There Such a Thing as Good Taste?]]></title>
        <id>http://www.paulgraham.com/goodtaste.html</id>
        <link href="http://www.paulgraham.com/goodtaste.html"/>
        <updated>2022-01-11T19:28:29.928Z</updated>
        <summary type="html"><![CDATA[November 2021
(This essay is derived from a talk at the Cambridge Union.)
When I was a kid, I'd have said there wasn't. My father told me so.
Some people like some things, and other people like other things,
and who's to say who's right?
It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that's what I'm going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there's no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.
We'd better start by saying what good taste is. There's a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kin…]]></summary>
        <author>
            <name>Paul Graham: Essays</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Open Philanthropy grant proposal: Causal representation learning of human preferences]]></title>
        <id>5BkEoJFEqQEWy9GcL</id>
        <link href="https://www.lesswrong.com/posts/5BkEoJFEqQEWy9GcL/an-open-philanthropy-grant-proposal-causal-representation"/>
        <updated>2022-01-11T11:28:02.000Z</updated>
        <summary type="html"><![CDATA[Published on January 11, 2022 11:28 AM GMT

This is a proposal I wrote for the recent Open Philanthropy call on AI Alignment projects. I am finishing my Ph.D. in quantum computing and I would like to start working on AI Alignment. Thus, this is my proposal for a postdoc-style grant to learn more about one area I think the community could pay more attention to. However, since I am quite new to this area, there might be things that I have overlooked. For that reason, if you see something that doesn't sound promising, or you disagree with it, I would be happy to know about it.
The proposal received some light comments from Victor Veitch, Ryan Carey, and José Hernández-Orallo. I also received some comments from Jaime Sevilla during the ideation phase. I am really grateful to all of them for th…]]></summary>
        <author>
            <name>PabloAMC</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emails from your Gratitude Journal]]></title>
        <id>FG7mgPFytq2a3rjJq</id>
        <link href="https://www.lesswrong.com/posts/FG7mgPFytq2a3rjJq/emails-from-your-gratitude-journal"/>
        <updated>2022-01-11T06:05:28.000Z</updated>
        <summary type="html"><![CDATA[Published on January 11, 2022 6:05 AM GMT

Lots has been written here about gratitude journaling.
In spite of knowing about the benefits, I've never managed to make the habit stick. I do, however, already have a very strong habit of responding to email and keeping my inbox under control.
That's what pushed me to build Email Notebook, which I'm hopeful will facilitate other folks' writing habits, too.
It's beta quality at the moment, but my email is on the contact page and I plan to put some effort into feature requests if there are any common themes. Already on the roadmap is to improve the variety of prompts, touching on some of the specific practices outlined by @david_gross.
If you do give it a try, please report any bugs you encounter :)
Discuss]]></summary>
        <author>
            <name>karlkeefer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibration proverbs]]></title>
        <id>NYH8kjdpLCTXcit8C</id>
        <link href="https://www.lesswrong.com/posts/NYH8kjdpLCTXcit8C/calibration-proverbs"/>
        <updated>2022-01-11T05:11:53.000Z</updated>
        <summary type="html"><![CDATA[Published on January 11, 2022 5:11 AM GMT

Cross-post from Telescopic Turnip.
Every problem is a calibration problem. That’s why most advice is basically useless: it tells you what to do, but doesn’t tell you when to stop. Therefore, the best pieces of advice are calibration advice. What we need is a metric to know if we are doing well, or if we should change our habits (and in which direction). For example, here are two poor pieces of advice:
"You should spend more time reading blogs, because compared to traditional media, bloggers have more freedom to communicate in original ways, and are more accountable when they say something false."
"You should spend less time reading blogs, because keeping up with a big pile of subscriptions takes a lot of time and makes you anxious."
Neither of the…]]></summary>
        <author>
            <name>Elmer of Malmesbury</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D&D.Sci Holiday Special: How the Grinch Pessimized Christmas Evaluation & Ruleset]]></title>
        <id>tQQaAuTtMCkYBbZcj</id>
        <link href="https://www.lesswrong.com/posts/tQQaAuTtMCkYBbZcj/d-and-d-sci-holiday-special-how-the-grinch-pessimized-1"/>
        <updated>2022-01-11T01:29:59.000Z</updated>
        <summary type="html"><![CDATA[Published on January 11, 2022 1:29 AM GMT

This is a follow-up to last week's D&D.Sci scenario: if you intend to play that, and haven't done so yet, you should do so now before spoiling yourself.
Full generation code is available here if you are interested, or you can read on.
RULESET
Each of the six toys makes a different amount of noise depending on what child receives it:

A Blum-Blooper makes 6 noise.
A Fum-Foozler is more popular with female Who children - it makes 8 noise with a female Who child but only 4 noise with a male one.
A Gah-Ginka is very noisy, but too simple to hold the attention of older children for long.  It makes 11 noise - 1/2 (rounded down) the child's age (so anywhere from 11 for an Age 1 child to 5 for an Age 12 child).
A Sloo-Slonker is complicated, and small Who…]]></summary>
        <author>
            <name>aphyer</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No Abstraction Without a Goal]]></title>
        <id>F2LS4tjbNDnLAsdbp</id>
        <link href="https://www.lesswrong.com/posts/F2LS4tjbNDnLAsdbp/no-abstraction-without-a-goal-1"/>
        <updated>2022-01-10T21:28:58.000Z</updated>
        <summary type="html"><![CDATA[Published on January 10, 2022 9:28 PM GMT

Abstractions are important.

In order to think in terms of "trees" and "birds" instead of raw retinal activation rates, human minds need to form abstractions.
In order to keep Nature red in tooth and claw, organisms must first evolve abstractions like "tooth" and "claw".
Neural networks can learn human abstractions.
Figuring out abstraction is possibly "the main foundational piece" of successful AI alignment.
Abstractions are functions that map a high-dimensional space to a low-dimensional space. They have more possible inputs than possible outputs, so abstractions have to shed some information.
Abstractions filter out useless information, while keeping useful information.
In order for there to be such a thing as "useful information", there must be some goal(s) being pursued.
You might argue that "abstraction" only means preserving information used to predict faraway observations from a given system. However, coarse modeling of distant objects is often a convergent subgoal of the kind of organisms that Nature selects for.
The scout does not tell the general about the bluejays he saw. He reports the number of bombers in the enemy's hangar. Condensation of information always selects for goal-relevant information. Any condensation of information implies that the omitted information is less goal-relevant than the reported information; there is no abstraction without a goal.
Discuss]]></summary>
        <author>
            <name>dkirmani</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is state a (semi-) rational agent?]]></title>
        <id>kwxDe6ACmepiJEzEj</id>
        <link href="https://www.lesswrong.com/posts/kwxDe6ACmepiJEzEj/is-state-a-semi-rational-agent"/>
        <updated>2022-01-10T19:53:25.000Z</updated>
        <summary type="html"><![CDATA[Published on January 10, 2022 7:53 PM GMT

Look at the following statements*:
- “India and Pakistan can agree on one thing: neither wants the other one around.”
- “The Americans don’t really want to fight for the South Koreans, but nor can they afford to be seen to be giving up on a friend.”
- “What is now the EU was set up so that France and Germany could hug each other so tightly in a loving embrace that neither would be able to get an arm free with which to punch the other.”
I don't want to argue whether these statements are true or not. What's bugging me is that they all contain an implicit assumption that a state acts as an intelligent agent at least and as a rational agent at most**.
Is this assumption true?
A state is a complex system which comprise civilians, military, businesses and different government branches. These parts may have different and even opposite goals and values and they can constrain each other. Will interactions between those parts lead to a (more or less) rational behavior of a whole?
Do you know a good book or article which reflects on this issue?
 
---
* Quotes from "Prisoners of Geography" by Tim Marshall
** I guess many people assume that states are human-like rational

Discuss]]></summary>
        <author>
            <name>Birke</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The First Person And The Physical Person]]></title>
        <id>tkhQC9YBgQAcgBLRP</id>
        <link href="https://www.lesswrong.com/posts/tkhQC9YBgQAcgBLRP/the-first-person-and-the-physical-person"/>
        <updated>2022-01-10T19:47:51.000Z</updated>
        <summary type="html"><![CDATA[Published on January 10, 2022 7:47 PM GMT

This is another attempt to promote my approach to anthropic paradoxes. I framed the argument differently hopefully making it more clear. Please refer to my website for more. 
What "I" Means
In the strict indexical sense, "I" just means the first person. However we also often interpret it as the physical person uttering the word and usually equate the two meanings, "I am this physical person" after all. But there is a caveat. The equivalency is bound by the perspective. 
The first person meaning of "I" is restricted to its own viewpoint, from other perspectives only the physical meaning can be used. If I hear you say "I'm hungry", to me the only correct way of interpreting it would be the physical person "[insert reader name] is hungry", not that "…]]></summary>
        <author>
            <name>dadadarren</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Newsletter: December 2021]]></title>
        <id>MDfesb7oYu8KhGvLR</id>
        <link href="https://www.lesswrong.com/posts/MDfesb7oYu8KhGvLR/forecasting-newsletter-december-2021"/>
        <updated>2022-01-10T19:35:41.000Z</updated>
        <summary type="html"><![CDATA[Published on January 10, 2022 7:35 PM GMT

Highlights

Polymarket’s future is uncertain after it settled with the CFTC for $1.4M
Astral Codex Ten gives out $40k to forecasting projects
Many people, including Mathew Yglesias, write predictions for 2022.
Eli Lifland writes the reference piece on bottlenecks to impactful forecasting
Google reveals the existence of a gigantic new internal prediction market
A new forecasting platform appears, Manifold Markets

Index

Prediction Markets & Forecasting Platforms
Long Content
Blog Posts
In The News

You can sign up for this newsletter on substack, or browse past newsletters here. If you have a content suggestion or want to reach out, you can leave a comment or find me on Twitter. A big hat tip goes to Nathan Young and Clay Graubard for comments and…]]></summary>
        <author>
            <name>NunoSempere</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Activated Charcoal for Hangover Prevention: Way more than you wanted to know]]></title>
        <id>LfrNFfJFcqnG9WuFf</id>
        <link href="https://www.lesswrong.com/posts/LfrNFfJFcqnG9WuFf/activated-charcoal-for-hangover-prevention-way-more-than-you"/>
        <updated>2022-01-10T19:26:54.000Z</updated>
        <summary type="html"><![CDATA[Published on January 10, 2022 7:26 PM GMT

Activated charcoal is charcoal that has been heated up a whole lot and then washed really hard, by water or acid. This process is what "activates" it, and activating it increases the surface area by a lot. I think it prevents hangovers.
I won't spend too much time in this post arguing that activated charcoal works. Instead, I'll mostly take for granted that it does, then take a tour of the various things that might cause hangovers, stopping at each to look at why activated charcoal might or might not be likely to reduce them. So even if you're sure activated charcoal is just a hokey health craze thing I've tricked myself into believing, this post still has lots of interesting hangover science you might not have heard about.
 
I used to suffer vici…]]></summary>
        <author>
            <name>Maxwell Peterson</name>
        </author>
    </entry>
</feed>