<!DOCTYPE html>
<html lang="en">

<head>
  <title>rss feed</title>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="noindex, nofollow" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
  <link rel="alternate" type="application/rss+xml" title="rss feed" href="feed.atom" />
  <link href="index.css" rel="stylesheet" />
  <style>
    :root {
        /**
     * Solarized Dark
     * Ethan Schoonover (modified by aramisgithub)
     */
        --base00: #002b36;
        --base01: #073642;
        --base02: #586e75;
        --base03: #657b83;
        --base04: #839496;
        --base05: #93a1a1;
        --base06: #eee8d5;
        --base07: #fdf6e3;
        --base08: #dc322f;
        --base09: #cb4b16;
        --base0A: #b58900;
        --base0B: #859900;
        --base0C: #2aa198;
        --base0D: #268bd2;
        --base0E: #6c71c4;
        --base0F: #d33682;

        /**
    * Typograph
    */
        --font-size-m: 1.6rem;
        --font-size-s: 1.4rem;

        /**
    * Components
    */
        --body-color: var(--base07);
        --body-bg: var(--base01);

        --daily-heading-color: var(--base03);

        --source-name-color: var(--base0F);
        --source-name-hover-color: var(--base0F);

        /* --card-bg: var(--base00); */
        --card-shadow: none;
        --card-radius: 0;
        /* --footer-link-hover-color: var(--base0D); */
    }

    .daily-heading {
        padding-left: 0;
    }

    .card {
        border: 1px solid var(--base02);
    }
</style>

</head>

<body>
<!-- %after-body-begin.html% -->
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2022-01-11">2022-01-11</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://www.paulgraham.com/articles.html">Paul Graham: Essays</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Project of One&#x27;s Own</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://www.paulgraham.com/own.html">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(8 min)</span>
                    <span>June 2021
A few days ago, on the way home from school, my nine year old son
told me he couldn&#x27;t wait to get home to write more of the story he
was working on. This made me as happy as anything I&#x27;ve heard him
say — not just because he was excited about his story, but because
he&#x27;d discovered this way of working. Working on a project of your
own is as different from ordinary work as skating is from walking.
It&#x27;s more fun, but also much more productive.
What proportion of great work has been done by people who were
skating in this sense? If not all of it, certainly a lot.
There is something special about working on a project of your own.
I wouldn&#x27;t say exactly that you&#x27;re happier. A better word would be
excited, or engaged. You&#x27;re happy when things are going well, but
often they aren&#x27;t. When I…</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weird Languages</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://www.paulgraham.com/weird.html">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(1 min)</span>
                    <span>August 2021
When people say that in their experience all programming languages
are basically equivalent, they&#x27;re making a statement not about
languages but about the kind of programming they&#x27;ve done.
99.5% of programming consists of gluing together calls to library
functions. All popular languages are equally good at this. So one
can easily spend one&#x27;s whole career operating in the intersection
of popular programming languages.
But the other .5% of programming is disproportionately interesting.
If you want to learn what it consists of, the weirdness of weird
languages is a good clue to follow.
Weird languages aren&#x27;t weird by accident. Not the good ones, at
least. The weirdness of the good ones usually implies the existence
of some form of programming that&#x27;s not just the usual gluing togeth…</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Work Hard</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://www.paulgraham.com/hwh.html">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(11 min)</span>
                    <span>June 2021
It might not seem there&#x27;s much to learn about how to work hard.
Anyone who&#x27;s been to school knows what it entails, even if they
chose not to do it. There are 12 year olds who work amazingly hard. And
yet when I ask if I know more about working hard now than when I
was in school, the answer is definitely yes.
One thing I know is that if you want to do great things, you&#x27;ll
have to work very hard. I wasn&#x27;t sure of that as a kid. Schoolwork
varied in difficulty; one didn&#x27;t always have to work super hard to
do well. And some of the things famous adults did, they seemed to
do almost effortlessly. Was there, perhaps, some way to evade hard
work through sheer brilliance? Now I know the answer to that question.
There isn&#x27;t.
The reason some subjects seemed easy was that my school had low
s…</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond Smart</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://www.paulgraham.com/smart.html">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(5 min)</span>
                    <span>October 2021
If you asked people what was special about Einstein, most would say
that he was really smart. Even the ones who tried to give you a
more sophisticated-sounding answer would probably think this first.
Till a few years ago I would have given the same answer myself. But
that wasn&#x27;t what was special about Einstein. What was special about
him was that he had important new ideas. Being very smart was a
necessary precondition for having those ideas, but the two are not
identical.
It may seem a hair-splitting distinction to point out that intelligence
and its consequences are not identical, but it isn&#x27;t. There&#x27;s a big
gap between them. Anyone who&#x27;s spent time around universities and
research labs knows how big. There are a lot of genuinely smart
people who don&#x27;t achieve very much.
I g…</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Is There Such a Thing as Good Taste?</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://www.paulgraham.com/goodtaste.html">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(4 min)</span>
                    <span>November 2021
(This essay is derived from a talk at the Cambridge Union.)
When I was a kid, I&#x27;d have said there wasn&#x27;t. My father told me so.
Some people like some things, and other people like other things,
and who&#x27;s to say who&#x27;s right?
It seemed so obvious that there was no such thing as good taste
that it was only through indirect evidence that I realized my father
was wrong. And that&#x27;s what I&#x27;m going to give you here: a proof by
reductio ad absurdum. If we start from the premise that there&#x27;s no
such thing as good taste, we end up with conclusions that are
obviously false, and therefore the premise must be wrong.
We&#x27;d better start by saying what good taste is. There&#x27;s a narrow
sense in which it refers to aesthetic judgements and a broader one
in which it refers to preferences of any kin…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2022-01-09">2022-01-09</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Observation of Vavilov Day</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/puYfAEJJomeodeSsi/an-observation-of-vavilov-day">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(10 min)</span>
                    <span>Published on January 3, 2022 9:10 PM GMT

I aspire to be a person who does good things, and who is capable of doing hard things in service of that. This is a plan to test that capacity.
I haven’t been in a battle, but if you gave me the choice between dying in battle and slowly starving to death, I would immediately choose battle. Battles are scary but they are short and then they are over.
If you gave me a chance to starve to death to generate some sufficiently good outcome, like saving millions of people from starvation, I think I would do it, and I would be glad to have the opportunity. It would hurt, but only for a few weeks, and in that time I could comfort myself with the warm glow of how good this was for other people.
If you gave me a chance to save millions of people by starving, …</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2022-01-06">2022-01-06</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">larger language models may disappoint you [or, an eternally unfinished draft]</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/pv7Qpu8WSge8NRbpB/larger-language-models-may-disappoint-you-or-an-eternally">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(54 min)</span>
                    <span>Published on November 26, 2021 11:08 PM GMT

what this post is
The following is an incomplete draft, which I&#x27;m publishing now because I am unlikely to ever finish writing it.
I no longer fully endorse all the claims in the post.  (In a few cases, I&#x27;ve added a note to say this explicitly.)  However, there are some arguments in the post that I still endorse, and which I have not seen made elsewhere.
This post is the result of me having lots of opinions about LM scaling, at various times in 2021, which were difficult to write down briefly or independently of one another.  This post, originally written in July 2021, is the closest I got to writing them all down in one place.
-nost, 11/26/21

0. caveat
This post will definitely disappoint you.
Or, anyway, it will definitely disappoint me.  I kn…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2022-01-03">2022-01-03</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ARC&#x27;s first technical report: Eliciting Latent Knowledge</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(44 min)</span>
                    <span>Published on December 14, 2021 8:09 PM GMT

ARC has published a report on Eliciting Latent Knowledge, an open problem which we believe is central to alignment. We think reading this report is the clearest way to understand what problems we are working on, how they fit into our plan for solving alignment in the worst case, and our research methodology.
The core difficulty we discuss is learning how to map between an AI’s model of the world and a human’s model. This is closely related to ontology identification (and other similar statements). Our main contribution is to present many possible approaches to the problem and a more precise discussion of why it seems to be difficult and important.
The report is available here as a google document. If you&#x27;re excited about this research, we&#x27;re hiring!
Q&amp;A
We&#x27;re particularly excited about answering questions posted here throughout December. We welcome any questions no matter how basic or confused; we would love to help people understand what research we’re doing and how we evaluate progress in enough detail that they could start to do it themselves.
Thanks to María Gutiérrez-Rojas for the illustrations in this piece (the good ones, blame us for the ugly diagrams). Thanks to Buck Shlegeris, Jon Uesato, Carl Shulman, and especially Holden Karnofsky for helpful discussions and comments.

Discuss</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-12-30">2021-12-30</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Omicron: My Current Model</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/zFhhDCxz87yKwqYQf/omicron-my-current-model">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(26 min)</span>
                    <span>Published on December 28, 2021 5:10 PM GMT

A year and a half ago, I wrote a post called Covid-19: My Current Model. Since then things have often changed, and we have learned a lot. It seems like high time for a new post of this type.
Note that this post mostly does not justify and explain its statements. I document my thinking, sources and analysis extensively elsewhere, little of this should be new.
This post combines the basic principles from my original post, which mostly still stand, with my core model for Omicron. I’ll summarize and update the first post, then share my current principles for Omicron and how to deal with and think about it.
There’s a lot of different things going on, so this will likely be incomplete, but hopefully it will prove useful. The personally useful executive…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-12-27">2021-12-27</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reply to Eliezer on Biological Anchors</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(48 min)</span>
                    <span>Published on December 23, 2021 4:15 PM GMT

The &quot;biological anchors&quot; method for forecasting transformative AI is the biggest non-trust-based input into my thinking about likely timelines for transformative AI. While I&#x27;m sympathetic to parts of Eliezer Yudkowsky&#x27;s recent post on it, I overall disagree with the post, and think it&#x27;s easy to get a misimpression of the &quot;biological anchors&quot; report (which I&#x27;ll abbreviate as &quot;Bio Anchors&quot;) - and Open Philanthropy&#x27;s take on it - by reading it. 
This post has three sections:

Most of Eliezer&#x27;s critique seems directed at assumptions the report explicitly does not make about how transformative AI will be developed, and more broadly, about the connection between its (the report&#x27;s) compute estimates and all-things-considered AI timelines. One way of put…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-12-22">2021-12-22</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reneging Prosocially</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/sjRG35aq5fosJ6mdG/reneging-prosocially">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(23 min)</span>
                    <span>Published on November 30, 2021 7:01 PM GMT

Author&#x27;s note: this essay was originally published elsewhere in 2019. It&#x27;s now being permanently rehosted at this link. I&#x27;ll be rehosting a small number of other upper-quintile essays from my past writing over the coming weeks.

Quite often, I will make an agreement, and then find myself regretting it. I’ll commit to spending a certain amount of hours helping someone with their problem, or I’ll agree to take part in an outing or a party or a project, or I’ll trade some item for a certain amount of value in return, and then later find that my predictions about how I would feel were pretty far off, and I&#x27;m unhappy.
Sometimes, I just suck it up and stick it out. But sometimes I renege on the agreement. In this essay, I’d like to lay out a model for …</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-12-19">2021-12-19</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Plan</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(48 min)</span>
                    <span>Published on December 10, 2021 11:41 PM GMT

This is a high-level overview of the reasoning behind my research priorities, written as a Q&amp;A.
What’s your plan for AI alignment?
Step 1: sort out our fundamental confusions about agency
Step 2: ambitious value learning (i.e. build an AI which correctly learns human values and optimizes for them)
Step 3: …
Step 4: profit!
… and do all that before AGI kills us all.
That sounds… awfully optimistic. Do you actually think that’s viable?
Better than a 50/50 chance of working in time.
Do you just have really long timelines?
No. My median is maybe 10-15 years, though that’s more a gut estimate based on how surprised I was over the past decade rather than a carefully-considered analysis. (I wouldn’t be shocked by another AI winter, especially on an ins…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-12-15">2021-12-15</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="https://www.lesswrong.com">LessWrong</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COVID and the holidays</summary>
                <a class="article-summary-link article-summary-box-outer" href="https://www.lesswrong.com/posts/GzzJZmqxcqg5KFf8r/covid-and-the-holidays">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(17 min)</span>
                    <span>Published on December 8, 2021 11:13 PM GMT

6 months ago I wrote about how 30-year-olds should basically go back to normal and no longer take many COVID precautions.
The holidays make this policy somewhat worse, because you can infect your family members. Further, vaccine efficacy has waned about 3x for most people since it’s been about six months since the vaccine wave. Some people have expressed concern about solstice in particular, which seems to turn out relatively safe. So let’s examine how you might navigate this.
The key takeaway is that a 1% chance of having COVID, which is about the base rate of COVID in the US, costs older relatives a few days of life if you pass it on to them. Thus, you should probably take a few easy actions beforehand to bring this down to a small number of ho…</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>

  <footer>
    <time id="build-timestamp" datetime="2022-01-11T19:56:44.667Z">2022-01-11T19:56:44.667Z</time>
    <span><a class="footer-link" href="https://github.com/osmoscraft/osmosfeed">osmosfeed 1.12.0</a></span>
  </footer>
  <script src="index.js"></script>
  <!-- %before-body-end.html% -->
</body>

</html>